
@inproceedings{yang_towards_2018,
	address = {New York, NY, USA},
	series = {{MSR} '18},
	title = {Towards extracting web {API} specifications from documentation},
	isbn = {978-1-4503-5716-6},
	url = {https://doi.org/10.1145/3196398.3196411},
	doi = {10.1145/3196398.3196411},
	abstract = {Web API specifications are machine-readable descriptions of APIs. These specifications, in combination with related tooling, simplify and support the consumption of APIs. However, despite the increased distribution of web APIs, specifications are rare and their creation and maintenance heavily rely on manual efforts by third parties. In this paper, we propose an automatic approach and an associated tool called D2Spec for extracting significant parts of such specifications from web API documentation pages. Given a seed online documentation page of an API, D2Spec first crawls all documentation pages on the API, and then uses a set of machine-learning techniques to extract the base URL, path templates, and HTTP methods - collectively describing the endpoints of the API. We evaluate whether D2Spec can accurately extract endpoints from documentation on 116 web APIs. The results show that D2Spec achieves a precision of 87.1\% in identifying base URLs, a precision of 80.3\% and a recall of 80.9\% in generating path templates, and a precision of 83.8\% and a recall of 77.2\% in extracting HTTP methods. In addition, in an evaluation on 64 APIs with pre-existing API specifications, D2Spec revealed many inconsistencies between web API documentation and their corresponding publicly available specifications. API consumers would benefit from D2Spec pointing them to, and allowing them thus to fix, such inconsistencies.},
	urldate = {2023-12-04},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Jinqiu and Wittern, Erik and Ying, Annie T. T. and Dolby, Julian and Tan, Lin},
	month = may,
	year = {2018},
	pages = {454--464},
}

@inproceedings{arora_simple_2019,
	title = {A simple but tough-to-beat baseline for sentence embeddings},
	url = {https://collaborate.princeton.edu/en/publications/a-simple-but-tough-to-beat-baseline-for-sentence-embeddings},
	language = {English (US)},
	urldate = {2024-02-22},
	author = {Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
	month = jan,
	year = {2019},
        booktitle = {5th International Conference on Learning Representations, ICLR 2017
},
	file = {Snapshot:/Users/edoriggio/Zotero/storage/VZFEBRIJ/a-simple-but-tough-to-beat-baseline-for-sentence-embeddings.html:text/html},
}

@inproceedings{lu_comparing_2024,
	address = {Avila, Spain},
	title = {Comparing the {Similarity} of {OpenAPI}-{Based} {Microservices}},
	abstract = {Microservices constitute the state of the art for implementing distributed systems and have been seen as a potential solution towards open systems. The characteristics of open systems require structured microservice management, including grouping microservices that are functionally similar. Microservices use RESTful APIs, often documented via OpenAPI specifications, to demonstrate their functionalities. Existing similarity metrics for microservice APIs have primarily focused on individual RESTful endpoints. However, understanding the full functionality of a microservice within an open system requires that the entirety of its OpenAPI documentation be considered. Thus, an approach that can compute a measure of similarity between entire microservice definitions in open environments is needed. In this paper, we propose an approach that can extract key information from the OpenAPI descriptions of microservices using Natural Language Processing (NLP) techniques, vectorise the extracted information using GLoVe embeddings, and cluster similar microservices using embedded API file vectors. Evaluations were conducted on real-world OpenAPI documents to demonstrate the effectiveness of the proposed approach.},
	language = {en},
	booktitle = {Proceedings of the 39th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	author = {Lu, Zhongyi and Delaney, Declan T and Lillis, David},
	year = {2024},
	file = {Lu et al. - 2024 - Comparing the Similarity of OpenAPI-Based Microser.pdf:/Users/edoriggio/Zotero/storage/3S9D6NYP/Lu et al. - 2024 - Comparing the Similarity of OpenAPI-Based Microser.pdf:application/pdf},
}

@misc{preston-werner_semantic_nodate,
	title = {Semantic {Versioning} 2.0.0},
	note = {\url{https://semver.org/}},
	abstract = {Semantic Versioning spec and website},
	language = {en},
	urldate = {2024-04-19},
	journal = {Semantic Versioning},
	author = {Preston-Werner, Tom},
	file = {Snapshot:/Users/edoriggio/Zotero/storage/Y3S6WPH2/semver.org.html:text/html},
}

@inproceedings{yasmin_first_2020,
	title = {A {First} {Look} at the {Deprecation} of {RESTful} {APIs}: {An} {Empirical} {Study}},
	shorttitle = {A {First} {Look} at the {Deprecation} of {RESTful} {APIs}},
	url = {https://ieeexplore.ieee.org/abstract/document/9240687},
	doi = {10.1109/ICSME46990.2020.00024},
	abstract = {REpresentational State Transfer (REST) is considered as one standard software architectural style to build web APIs that can integrate software systems over the internet. However, while connecting systems, RESTful APIs might also break the dependent applications that rely on their services when they introduce breaking changes, e.g., an older version of the API is no longer supported. To warn developers promptly and thus prevent critical impact on downstream applications, a deprecated-removed model should be followed, and deprecation-related information such as alternative approaches should also be listed. While API deprecation analysis as a theme is not new, most existing work focuses on non-web APIs, such as the ones provided by Java and Android.To investigate RESTful API deprecation, we propose a framework called RADA (RESTful API Deprecation Analyzer). RADA is capable of automatically identifying deprecated API elements and analyzing impacted operations from an OpenAPI specification, a machine-readable profile for describing RESTful web service. We apply RADA on 2,224 OpenAPI specifications of 1,368 RESTful APIs collected from APIs.guru, the largest directory of OpenAPI specifications. Based on the data mined by RADA, we perform an empirical study to investigate how the deprecated-removed protocol is followed in RESTful APIs and characterize practices in RESTful API deprecation. The results of our study reveal several severe deprecation-related problems in existing RESTful APIs. Our implementation of RADA and detailed empirical results are publicly available for future intelligent tools that could automatically identify and migrate usage of deprecated RESTful API operations in client code.},
	urldate = {2024-04-23},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Yasmin, Jerin and Tian, Yuan and Yang, Jinqiu},
	month = sep,
	year = {2020},
	note = {ISSN: 2576-3148},
	keywords = {OpenAPI Specification, Web API, Web services, API Deprecation, Evolution of Web APIs, Representational state transfer, Restful API, RESTful API, Software maintenance, Software systems, Standards, Tools},
	pages = {151--161},
	file = {IEEE Xplore Abstract Record:/Users/edoriggio/Zotero/storage/9HW9SL2Q/9240687.html:text/html;IEEE Xplore Full Text PDF:/Users/edoriggio/Zotero/storage/URVAFRZS/Yasmin et al. - 2020 - A First Look at the Deprecation of RESTful APIs A.pdf:application/pdf},
}

@article{mohammed_state---art_2021,
	title = {A state-of-the-art survey on semantic similarity for document clustering using {GloVe} and density-based algorithms},
	volume = {22},
	copyright = {Copyright (c) 2021 Institute of Advanced Engineering and Science},
	issn = {2502-4760},
	url = {https://ijeecs.iaescore.com/index.php/IJEECS/article/view/23698},
	doi = {10.11591/ijeecs.v22.i1.pp552-562},
	abstract = {Semantic similarity is the process of identifying relevant data semantically. The traditional way of identifying document similarity is by using synonymous keywords and syntactician. In comparison, semantic similarity is to find similar data using meaning of words and semantics. Clustering is a concept of grouping objects that have the same features and properties as a cluster and separate from those objects that have different features and properties. In semantic document clustering, documents are clustered using semantic similarity techniques with similarity measurements. One of the common techniques to cluster documents is the density-based clustering algorithms using the density of data points as a main strategic to measure the similarity between them. In this paper, a state-of-the-art survey is presented to analyze the density-based algorithms for clustering documents. Furthermore, the similarity and evaluation measures are investigated with the selected algorithms to grasp the common ones. The delivered review revealed that the most used density-based algorithms in document clustering are DBSCAN and DPC. The most effective similarity measurement has been used with density-based algorithms, specifically DBSCAN and DPC, is Cosine similarity with F-measure for performance and accuracy evaluation.},
	language = {en},
	number = {1},
	urldate = {2024-04-23},
	journal = {Indonesian Journal of Electrical Engineering and Computer Science},
	author = {Mohammed, Shapol M. and Jacksi, Karwan and Zeebaree, Subhi R. M.},
	month = apr,
	year = {2021},
	note = {Number: 1},
	keywords = {DBSCAN, Density-based algorithm, DPC GloVe word embedding, Evaluation Measurement, Similarity Measurement},
	pages = {552--562},
	file = {Full Text PDF:/Users/edoriggio/Zotero/storage/JNKDMG6E/Mohammed et al. - 2021 - A state-of-the-art survey on semantic similarity f.pdf:application/pdf},
}

@misc{mu_all-but--top_2018,
	title = {All-but-the-{Top}: {Simple} and {Effective} {Postprocessing} for {Word} {Representations}},
	shorttitle = {All-but-the-{Top}},
	url = {http://arxiv.org/abs/1702.01417},
	doi = {10.48550/arXiv.1702.01417},
	abstract = {Real-valued word representations have transformed NLP applications; popular examples are word2vec and GloVe, recognized for their ability to capture linguistic regularities. In this paper, we demonstrate a \{{\textbackslash}em very simple\}, and yet counter-intuitive, postprocessing technique -- eliminate the common mean vector and a few top dominating directions from the word vectors -- that renders off-the-shelf representations \{{\textbackslash}em even stronger\}. The postprocessing is empirically validated on a variety of lexical-level intrinsic tasks (word similarity, concept categorization, word analogy) and sentence-level tasks (semantic textural similarity and \{ text classification\}) on multiple datasets and with a variety of representation methods and hyperparameter choices in multiple languages; in each case, the processed representations are consistently better than the original ones.},
	urldate = {2024-04-23},
	publisher = {arXiv},
	author = {Mu, Jiaqi and Bhat, Suma and Viswanath, Pramod},
	month = mar,
	year = {2018},
	note = {arXiv:1702.01417 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/edoriggio/Zotero/storage/QGCAMRZY/Mu et al. - 2018 - All-but-the-Top Simple and Effective Postprocessi.pdf:application/pdf;arXiv.org Snapshot:/Users/edoriggio/Zotero/storage/SNI3WZ8Q/1702.html:text/html},
}

@misc{wei_survey_2023,
	title = {A {Survey} on {Query}-based {API} {Recommendation}},
	url = {https://ui.adsabs.harvard.edu/abs/2023arXiv231210623W},
	doi = {10.48550/arXiv.2312.10623},
	abstract = {Application Programming Interfaces (APIs) are designed to help developers build software more effectively. Recommending the right APIs for specific tasks has gained increasing attention among researchers and developers in recent years. To comprehensively understand this research domain, we have surveyed to analyze API recommendation studies published in the last 10 years. Our study begins with an overview of the structure of API recommendation tools. Subsequently, we systematically analyze prior research and pose four key research questions. For RQ1, we examine the volume of published papers and the venues in which these papers appear within the API recommendation field. In RQ2, we categorize and summarize the prevalent data sources and collection methods employed in API recommendation research. In RQ3, we explore the types of data and common data representations utilized by API recommendation approaches. We also investigate the typical data extraction procedures and collection approaches employed by the existing approaches. RQ4 delves into the modeling techniques employed by API recommendation approaches, encompassing both statistical and deep learning models. Additionally, we compile an overview of the prevalent ranking strategies and evaluation metrics used for assessing API recommendation tools. Drawing from our survey findings, we identify current challenges in API recommendation research that warrant further exploration, along with potential avenues for future research.},
	urldate = {2024-04-23},
	author = {Wei, Moshi and Shiri Harzevili, Nima and Boaye Belle, Alvine and Wang, Junjie and Shi, Lin and Yang, Jinqiu and Wang, Song and Zhen, Ming and {Jiang}},
	month = dec,
	year = {2023},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2023arXiv231210623W},
	keywords = {Computer Science - Software Engineering, Computer Science - Information Retrieval},
	file = {Full Text PDF:/Users/edoriggio/Zotero/storage/DL5MPUQK/Wei et al. - 2023 - A Survey on Query-based API Recommendation.pdf:application/pdf},
}

@inproceedings{kim_empirical_2019,
	title = {An {Empirical} {Analysis} of {GraphQL} {API} {Schemas} in {Open} {Code} {Repositories} and {Package} {Registries}},
	url = {https://urn.kb.se/resolve?urn=urn:nbn:se:liu:diva-168651},
	abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
	language = {eng},
	urldate = {2024-04-23},
	author = {Kim, Yun Wan and Consens, Mariano P. and Hartig, Olaf},
	year = {2019},
	file = {Snapshot:/Users/edoriggio/Zotero/storage/BIPBQ2RJ/record.html:text/html},
}

@inproceedings{assefi_intelligent_2022,
	title = {An {Intelligent} {Data}-{Centric} {Web} {Crawler} {Service} for {API} {Corpus} {Construction} at {Scale}},
	url = {https://ieeexplore.ieee.org/abstract/document/9885268},
	doi = {10.1109/ICWS55610.2022.00064},
	abstract = {The number of web APIs is growing rapidly. API adoption is increasing across all industries with executives prioritizing investments in the API economy. Each API provider offers API documentation which includes complex descriptions. In order to collect and understand the applications and operations of diverse APIs, software engineers read lengthy and complicated API documentations. Understanding the variety of API documentations is a labor intensive and error-prone process. In this paper, we introduce a data-centric web crawler service to collect, analyze, and construct a large corpus of API documentations. The generated API Corpus can be used in machine programming (i.e., code generation, code search). The proposed API web-crawler intelligently harvests more than 2.8M API documentation pages where it uses a machine-learning-based approach with an accuracy of 91.32\% to select only web API pages (REST). We also conducted an extensive and end-to-end real-world evaluation, where the proposed API web-crawler not only collects a sheer number of API pages, but also successfully validates 1,222 APIs out of 1,521 target APIs with a success rate of 80.34\%.},
	urldate = {2024-04-23},
	booktitle = {2022 {IEEE} {International} {Conference} on {Web} {Services} ({ICWS})},
	author = {Assefi, Mehdi and Bahrami, Mehdi and Arora, Sarthak and Taha, Thiab R. and Arabnia, Hamid R. and Rasheed, Khaled M. and Chen, Wei-Peng},
	month = jul,
	year = {2022},
	keywords = {Biological system modeling, Codes, Crawlers, Documentation, Filtering algorithms, machine-learning, Programming, Web API, Web Crawler, Web services},
	pages = {385--390},
	file = {IEEE Xplore Abstract Record:/Users/edoriggio/Zotero/storage/DCLG6QWK/9885268.html:text/html;IEEE Xplore Full Text PDF:/Users/edoriggio/Zotero/storage/26CEC77R/Assefi et al. - 2022 - An Intelligent Data-Centric Web Crawler Service fo.pdf:application/pdf},
}

@incollection{de_api_2017,
	address = {Berkeley, CA},
	title = {{API} {Documentation}},
	isbn = {978-1-4842-1305-6},
	url = {https://doi.org/10.1007/978-1-4842-1305-6_4},
	abstract = {Documenting a REST API is important for its successful adoption. APIs expose data and services that consumers want to use. An API should be designed with an interface that the consumer can understand. API documentation is key to the app developers comprehending the API. The documentation should help the developer to learn about the API functionality and enable them to start using it easily. This chapter looks at the aspects of documenting an API and some of the tools and technologies available for API documentation, including RAML, Swagger, API Blueprint, and others.},
	language = {en},
	urldate = {2024-04-23},
	booktitle = {{API} {Management}: {An} {Architect}'s {Guide} to {Developing} and {Managing} {APIs} for {Your} {Organization}},
	publisher = {Apress},
	author = {De, Brajesh},
	editor = {De, Brajesh},
	year = {2017},
	doi = {10.1007/978-1-4842-1305-6_4},
	pages = {59--80},
}

@inproceedings{ma_api_2020,
	address = {Cham},
	title = {{API} {Prober} – {A} {Tool} for {Analyzing} {Web} {API} {Features} and {Clustering} {Web} {APIs}},
	isbn = {978-3-030-34986-8},
	doi = {10.1007/978-3-030-34986-8_6},
	abstract = {Nowadays, Web services attract more and more attentions. Many companies expose their data or services by publishing Web APIs (Application Programming Interface) to let users create innovative services or applications. To ease the use of various and complex APIs, multiple API directory services or API search engines, such as Mashape, API Harmony, and ProgrammableWeb, are emerging in recent years. However, most API systems are only able to help developers to understand Web APIs. Furthermore, these systems do neither provide usage examples for users, nor help users understand the “closeness” between APIs. Therefore, we propose a system, referred to as API Prober, to address the above issues by constructing an API “dictionary”. There are multiple main features of API Prober. First, API Prober transforms OAS (OpenAPI Specification 2.0) into the graph structure in Neo4J database and annotates the semantic concepts on each graph node by using LDA (Latent Dirichlet Allocation) and WordNet. Second, by parsing source codes in the GitHub, API Prober is able to retrieve code examples that utilize APIs. Third, API Prober performs API classification through cluster analysis for OAS documents. Finally, the experimental results show that API Prober can appropriately produce service clusters.},
	language = {en},
	booktitle = {Advances in {E}-{Business} {Engineering} for {Ubiquitous} {Computing}},
	publisher = {Springer International Publishing},
	author = {Ma, Shang-Pin and Hsu, Ming-Jen and Chen, Hsiao-Jung and Su, Yu-Sheng},
	editor = {Chao, Kuo-Ming and Jiang, Lihong and Hussain, Omar Khadeer and Ma, Shang-Pin and Fei, Xiang},
	year = {2020},
	pages = {81--96},
}

@misc{moon_api-miner_2022,
	title = {{API}-{Miner}: an {API}-to-{API} {Specification} {Recommendation} {Engine}},
	shorttitle = {{API}-{Miner}},
	url = {https://ui.adsabs.harvard.edu/abs/2022arXiv221207253M},
	doi = {10.48550/arXiv.2212.07253},
	abstract = {When designing a new API for a large project, developers need to make smart design choices so that their code base can grow sustainably. To ensure that new API components are well designed, developers can learn from existing API components. However, the lack of standardized methods for comparing API designs makes this learning process time-consuming and difficult. To address this gap we developed API-Miner, to the best of our knowledge, one of the first API-to-API specification recommendation engines. API-Miner retrieves relevant specification components written in OpenAPI (a widely adopted language used to describe web APIs). API-miner presents several significant contributions, including: (1) novel methods of processing and extracting key information from OpenAPI specifications, (2) innovative feature extraction techniques that are optimized for the highly technical API specification domain, and (3) a novel log-linear probabilistic model that combines multiple signals to retrieve relevant and high quality OpenAPI specification components given a query specification. We evaluate API-Miner in both quantitative and qualitative tasks and achieve an overall of 91.7\% recall@1 and 56.2\% F1, which surpasses baseline performance by 15.4\% in recall@1 and 3.2\% in F1. Overall, API-Miner will allow developers to retrieve relevant OpenAPI specification components from a public or internal database in the early stages of the API development cycle, so that they can learn from existing established examples and potentially identify redundancies in their work. It provides the guidance developers need to accelerate development process and contribute thoughtfully designed APIs that promote code maintainability and quality. Code is available on GitHub at https://github.com/jpmorganchase/api-miner.},
	urldate = {2024-04-23},
	author = {Moon, Sae Young and Kerr, Gregor and Silavong, Fran and Moran, Sean},
	month = dec,
	year = {2022},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2022arXiv221207253M},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {Full Text PDF:/Users/edoriggio/Zotero/storage/GLBKWSEM/Moon et al. - 2022 - API-Miner an API-to-API Specification Recommendat.pdf:application/pdf},
}

@misc{dai_document_2015,
	title = {Document {Embedding} with {Paragraph} {Vectors}},
	url = {http://arxiv.org/abs/1507.07998},
	doi = {10.48550/arXiv.1507.07998},
	abstract = {Paragraph Vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts. In their work, the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis. That proof of concept, while encouraging, was rather narrow. Here we consider tasks other than sentiment analysis, provide a more thorough comparison of Paragraph Vectors to other document modelling algorithms such as Latent Dirichlet Allocation, and evaluate performance of the method as we vary the dimensionality of the learned representation. We benchmarked the models on two document similarity data sets, one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method performs significantly better than other methods, and propose a simple improvement to enhance embedding quality. Somewhat surprisingly, we also show that much like word embeddings, vector operations on Paragraph Vectors can perform useful semantic results.},
	urldate = {2024-04-23},
	publisher = {arXiv},
	author = {Dai, Andrew M. and Olah, Christopher and Le, Quoc V.},
	month = jul,
	year = {2015},
	note = {arXiv:1507.07998 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/edoriggio/Zotero/storage/QWIQXK3R/Dai et al. - 2015 - Document Embedding with Paragraph Vectors.pdf:application/pdf;arXiv.org Snapshot:/Users/edoriggio/Zotero/storage/IIF8UUIX/1507.html:text/html},
}

@article{malkov_efficient_2020,
	title = {Efficient and {Robust} {Approximate} {Nearest} {Neighbor} {Search} {Using} {Hierarchical} {Navigable} {Small} {World} {Graphs}},
	volume = {42},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/abstract/document/8594636},
	doi = {10.1109/TPAMI.2018.2889473},
	abstract = {We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW). The proposed solution is fully graph-based, without any need for additional search structures (typically used at the coarse search stage of the most proximity graph techniques). Hierarchical NSW incrementally builds a multi-layer structure consisting of a hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting the search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation.},
	number = {4},
	urldate = {2024-04-23},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Malkov, Yu A. and Yashunin, D. A.},
	month = apr,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Biological system modeling, approximate search, Approximation algorithms, artificial intelligence, big data, Brain modeling, Complexity theory, Data models, data structures, Graph and tree search strategies, graphs and networks, information search and retrieval, information storage and retrieval, information technology and systems, nearest neighbor search, Routing, Search problems, search process, similarity search},
	pages = {824--836},
	file = {IEEE Xplore Abstract Record:/Users/edoriggio/Zotero/storage/X45PHXRR/8594636.html:text/html;Submitted Version:/Users/edoriggio/Zotero/storage/F8IXDVK9/Malkov and Yashunin - 2020 - Efficient and Robust Approximate Nearest Neighbor .pdf:application/pdf},
}

@inproceedings{pennington_glove_2014,
	address = {Doha, Qatar},
	title = {{GloVe}: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {{GloVe}},
	url = {https://aclanthology.org/D14-1162},
	doi = {10.3115/v1/D14-1162},
	urldate = {2024-04-23},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	editor = {Moschitti, Alessandro and Pang, Bo and Daelemans, Walter},
	month = oct,
	year = {2014},
	pages = {1532--1543},
	file = {Full Text PDF:/Users/edoriggio/Zotero/storage/CSKYJBL3/Pennington et al. - 2014 - GloVe Global Vectors for Word Representation.pdf:application/pdf},
}

@article{peltonen_information_2015,
	title = {Information retrieval approach to meta-visualization},
	volume = {99},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-014-5464-x},
	doi = {10.1007/s10994-014-5464-x},
	abstract = {Visualization is crucial in the first steps of data analysis. In visual data exploration with scatter plots, no single plot is sufficient to analyze complicated high-dimensional data sets. Given numerous visualizations created with different features or methods, meta-visualization is needed to analyze the visualizations together. We solve how to arrange numerous visualizations onto a meta-visualization display, so that their similarities and differences can be analyzed. Visualization has recently been formalized as an information retrieval task; we extend this approach, and formalize meta-visualization as an information retrieval task whose performance can be rigorously quantified and optimized. We introduce a machine learning approach to optimize the meta-visualization, based on an information retrieval perspective: two visualizations are similar if the analyst would retrieve similar neighborhoods between data samples from either visualization. Based on the approach, we introduce a nonlinear embedding method for meta-visualization: it optimizes locations of visualizations on a display, so that visualizations giving similar information about data are close to each other. In experiments we show such meta-visualization outperforms alternatives, and yields insight into data in several case studies.},
	language = {en},
	number = {2},
	urldate = {2024-04-23},
	journal = {Machine Learning},
	author = {Peltonen, Jaakko and Lin, Ziyuan},
	month = may,
	year = {2015},
	keywords = {Meta-visualization, Neighbor embedding, Nonlinear dimensionality reduction},
	pages = {189--229},
	file = {Full Text PDF:/Users/edoriggio/Zotero/storage/MFFUP7Z8/Peltonen and Lin - 2015 - Information retrieval approach to meta-visualizati.pdf:application/pdf},
}

@article{cover_nearest_1967,
	title = {Nearest neighbor pattern classification},
	volume = {13},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/abstract/document/1053964},
	doi = {10.1109/TIT.1967.1053964},
	abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of errorRof such a rule must be at least as great as the Bayes probability of errorR{\textasciicircum}{\textbackslash}ast–the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in theM-category case thatR{\textasciicircum}{\textbackslash}ast łeq R łeq R{\textasciicircum}{\textbackslash}ast(2 –MR{\textasciicircum}{\textbackslash}ast/(M-1)), where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
	number = {1},
	urldate = {2024-04-23},
	journal = {IEEE Transactions on Information Theory},
	author = {Cover, T. and Hart, P.},
	month = jan,
	year = {1967},
	pages = {21--27},
	file = {IEEE Xplore Abstract Record:/Users/edoriggio/Zotero/storage/4NRFETML/1053964.html:text/html;IEEE Xplore Full Text PDF:/Users/edoriggio/Zotero/storage/PDQRGXFU/Cover and Hart - 1967 - Nearest neighbor pattern classification.pdf:application/pdf},
}

@misc{noauthor_openapi_nodate,
	title = {{OpenAPI} {Specification} v3.1.0 {\textbar} {Introduction}, {Definitions}, \& {More}},
	note = {\url{https://spec.openapis.org/oas/v3.1.0}},
	urldate = {2024-04-23},
	file = {OpenAPI Specification v3.1.0 | Introduction, Definitions, & More:/Users/edoriggio/Zotero/storage/KTH2CQ3E/v3.1.html:text/html},
}

@inproceedings{tsai_rest_2021,
	title = {{REST} {API} {Fuzzing} by {Coverage} {Level} {Guided} {Blackbox} {Testing}},
	url = {https://ieeexplore.ieee.org/abstract/document/9724904},
	doi = {10.1109/QRS54544.2021.00040},
	abstract = {With the growth of web applications, REST APIs have become the primary communication method between services. In order to ensure system reliability and security, software quality can be assured by effective testing methods. Black box fuzz testing is one of the effective methods to perform tests on a large scale. However, conventional black box fuzz testing generates random data without judging the quality of the input. We implement a black box fuzz testing method for REST APIs. It resolves the issues of blind mutations without knowing the effectiveness by Test Coverage Level feedback. We also enhance the mutation strategies by reducing the testing complexity for REST APIs, generating more appropriate test cases to cover possible paths. We evaluate our method by testing two large open-source projects and 89 bugs are reported and confirmed. In addition, we find 351 bugs from 64 remote API services in APIs.guru. The work is in https://github.com/iasthc/hsuan-fuzz.},
	urldate = {2024-04-23},
	booktitle = {2021 {IEEE} 21st {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} ({QRS})},
	author = {Tsai, Chung-Hsuan and Tsai, Shi-Chun and Huang, Shih-Kun},
	month = dec,
	year = {2021},
	note = {ISSN: 2693-9177},
	keywords = {Codes, Complexity theory, Black-box testing, Computer bugs, Fuzz testing, Fuzzing, OpenAPI, REST API, Security, Software quality, Software reliability, Test coverage level},
	pages = {291--300},
	file = {IEEE Xplore Abstract Record:/Users/edoriggio/Zotero/storage/4PH227LY/9724904.html:text/html;IEEE Xplore Full Text PDF:/Users/edoriggio/Zotero/storage/AUW85EEM/Tsai et al. - 2021 - REST API Fuzzing by Coverage Level Guided Blackbox.pdf:application/pdf},
}

@article{kotstein_restberta_2024,
	title = {{RESTBERTa}: a {Transformer}-based question answering approach for semantic search in {Web} {API} documentation},
	issn = {1573-7543},
	shorttitle = {{RESTBERTa}},
	url = {https://doi.org/10.1007/s10586-023-04237-x},
	doi = {10.1007/s10586-023-04237-x},
	abstract = {To enable machines to process state-of-practice Web API documentation, we propose a Transformer model for the generic task of identifying a Web API element within a syntax structure that matches a natural language query. We solve this semantic-search task with Transformer-based question answering and demonstrate the applicability of our approach to two different tasks, namely the discovery of endpoints and the identification of parameters in payload schemas. With samples from 2321 OpenAPI documentation, we prepare different datasets and fine-tune pre-trained BERT models to these two tasks. We evaluate the generalizability and the robustness of our fine-tuned models. We achieve accuracies of 81.95\% for the parameter-matching and 88.44\% for the endpoint-discovery task.},
	language = {en},
	urldate = {2024-04-23},
	journal = {Cluster Computing},
	author = {Kotstein, Sebastian and Decker, Christian},
	month = jan,
	year = {2024},
	keywords = {BERT, Endpoint discovery, Parameter matching, Question answering, Semantic search, Web API documentation},
	file = {Full Text PDF:/Users/edoriggio/Zotero/storage/D9JA226W/Kotstein and Decker - 2024 - RESTBERTa a Transformer-based question answering .pdf:application/pdf},
}

@article{ma_restful_2023,
	title = {{RESTful} {API} {Analysis}, {Recommendation}, and {Client} {Code} {Retrieval}},
	volume = {12},
	copyright = {© 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
	url = {https://www.proquest.com/docview/2785187072/abstract/FEAB7B31351F44AFPQ/1},
	doi = {10.3390/electronics12051252},
	abstract = {Numerous companies create innovative software systems using Web APIs (Application Programming Interfaces). API search engines and API directory services, such as ProgrammableWeb, Rapid API Hub, APIs.guru, and API Harmony, have been developed to facilitate the utilization of various APIs. Unfortunately, most API systems provide only superficial support, with no assistance in obtaining relevant APIs or examples of code usage. To better realize the “FAIR” (Findability, Accessibility, Interoperability, and Reusability) features for the usage of Web APIs, in this study, we developed an API inspection system (referred to as API Prober) to provide a new API directory service with multiple supplemental functionalities. To facilitate the findability and accessibility of APIs, API Prober transforms OAS (OpenAPI Specifications) into a graph structure and automatically annotates the semantic concepts using LDA (Latent Dirichlet Allocation) and WordNet. To enhance interoperability, API Prober also classifies APIs by clustering OAS documents and recommends alternative services to be substituted or merged with the target service. Finally, to support reusability, API Prober makes it possible to retrieve examples of API utilization code in Java by parsing source code in GitHub. The experimental results demonstrate the effectiveness of the API Prober in recommending relevant services and providing usage examples based on real-world client code. This research contributes to providing viable methods to appropriately analyze and cluster Web APIs, and recommend APIs and client code examples.},
	language = {English},
	number = {5},
	urldate = {2024-04-23},
	journal = {Electronics},
	author = {Ma, Shang-Pin and Hsu, Ming-Jen and Hsiao-Jung, Chen and Lin, Chuan-Jie},
	year = {2023},
	note = {Num Pages: 1252
Place: Basel, Switzerland
Publisher: MDPI AG},
	keywords = {cluster analysis, code example, GitHub, Latent Dirichlet Allocation, OpenAPI Specification, service recommendation, Web API, Representational state transfer},
	pages = {1252},
	file = {Full Text PDF:/Users/edoriggio/Zotero/storage/ZYWGH4BS/Ma et al. - 2023 - RESTful API Analysis, Recommendation, and Client C.pdf:application/pdf},
}

@inproceedings{hinton_stochastic_2002,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'02},
	title = {Stochastic neighbor embedding},
	abstract = {We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to define a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low-dimensional "images" of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word "bank", to have versions close to the images of both "river" and "finance" without forcing the images of outdoor concepts to be located close to those of corporate concepts.},
	urldate = {2024-04-23},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Hinton, Geoffrey and Roweis, Sam},
	month = jan,
	year = {2002},
	pages = {857--864},
}

@misc{guo_testing_2022,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Testing and {Validating} the {Cosine} {Similarity} {Measure} for {Textual} {Analysis}},
	url = {https://papers.ssrn.com/abstract=4258463},
	note = {\url{https://dx.doi.org/10.2139/ssrn.4258463}},
	abstract = {Textual similarity has drawn much attention in the recent literature of accounting and related fields. There has been, however, limited work to systematically test and validate its measures. In this paper I conduct three incremental studies to comprehensively test and validate the commonly used cosine similarity (COS) method. The results show that the 5-gram COS measure (based on phrases of five consecutive words) is a viable approach to assessing textual similarity. In addition, I develop a new scale termed boundary n-gram (the longest match that can be found in two texts) to obtain complementary information about textual similarity.},
	language = {en},
	urldate = {2024-04-23},
	author = {Guo, Ken},
	month = oct,
	year = {2022},
	keywords = {Construct Validation, Cosine Similarity, n-gram, Textual Research Method, Textual Similarity},
	file = {Full Text PDF:/Users/edoriggio/Zotero/storage/JEY3592H/Guo - 2022 - Testing and Validating the Cosine Similarity Measu.pdf:application/pdf},
}

@misc{cer_universal_2018,
	title = {Universal {Sentence} {Encoder}},
	url = {https://ui.adsabs.harvard.edu/abs/2018arXiv180311175C},
	doi = {10.48550/arXiv.1803.11175},
	abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
	urldate = {2024-04-23},
	author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and St. John, Rhomni and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
	month = mar,
	year = {2018},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2018arXiv180311175C},
	keywords = {Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/edoriggio/Zotero/storage/46IG7RDK/Cer et al. - 2018 - Universal Sentence Encoder.pdf:application/pdf},
}

@article{maaten_visualizing_2008,
	title = {Visualizing {Data} using t-{SNE}},
	volume = {9},
	url = {http://jmlr.org/papers/v9/vandermaaten08a.html},
	abstract = {We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each
datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic
Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces
significantly better visualizations by reducing the tendency to crowd points together in the center
of the map. t-SNE is better than existing techniques at creating a single map that reveals structure
at many different scales. This is particularly important for high-dimensional data that lie on several
different, but related, low-dimensional manifolds, such as images of objects from multiple classes
seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how
t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the
data to influence the way in which a subset of the data is displayed. We illustrate the performance of
t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization
techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on
almost all of the data sets.},
	number = {86},
	journal = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	year = {2008},
	pages = {2579--2605},
	file = {Full Text:/Users/edoriggio/Zotero/storage/DBFLTUVR/Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf:application/pdf},
}

@inproceedings{souhaila_serbout_apistic_2024,
	address = {Lisbon, Portugal},
	title = {{APIstic}: {A} {Large} {Collection} of {OpenAPI} {Metrics}},
	booktitle = {Proc. 21st {IEEE}/{ACM} {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {{Souhaila Serbout} and {Cesare Pautasso}},
	month = apr,
	year = {2024},
}

@inproceedings{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships.  In this paper we present several improvements that make the Skip-gram model more expressive and enable it to learn higher quality vectors more rapidly.  We show that by subsampling frequent words we obtain significant speedup,  and also learn higher quality representations as measured by our tasks. We also introduce Negative Sampling, a simplified variant of Noise Contrastive Estimation (NCE) that learns more accurate vectors for frequent words compared to the hierarchical softmax.   An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases.  For example, the meanings of Canada'' and "Air'' cannot be easily combined to obtain "Air Canada''.  Motivated by this example, we present a simple and efficient method for finding phrases, and show that their vector representations can be accurately learned by the Skip-gram model. "},
	urldate = {2024-04-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} (NIPS 2013)},
	publisher = {Curran Associates, Inc.},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	year = {2013},
	file = {Full Text PDF:/Users/edoriggio/Zotero/storage/ZWN3334G/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2024-04-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:/Users/edoriggio/Zotero/storage/PB5GXBQ8/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@inproceedings{iyyer_deep_2015,
	address = {Beijing, China},
	title = {Deep {Unordered} {Composition} {Rivals} {Syntactic} {Methods} for {Text} {Classification}},
	url = {https://aclanthology.org/P15-1162},
	doi = {10.3115/v1/P15-1162},
	urldate = {2024-04-24},
	booktitle = {Proceedings of the 53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Iyyer, Mohit and Manjunatha, Varun and Boyd-Graber, Jordan and Daumé III, Hal},
	editor = {Zong, Chengqing and Strube, Michael},
	month = jul,
	year = {2015},
	pages = {1681--1691},
	file = {Full Text PDF:/Users/edoriggio/Zotero/storage/Q5HJCTKT/Iyyer et al. - 2015 - Deep Unordered Composition Rivals Syntactic Method.pdf:application/pdf},
}

@article{navarro_searching_2002,
	title = {Searching in metric spaces by spatial approximation},
	volume = {11},
	issn = {0949-877X},
	url = {https://doi.org/10.1007/s007780200060},
	doi = {10.1007/s007780200060},
	abstract = {We propose a new data structure to search in metric spaces. A metric space is formed by a collection of objects and a distance function defined among them which satisfies the triangle inequality. The goal is, given a set of objects and a query, retrieve those objects close enough to the query. The complexity measure is the number of distances computed to achieve this goal. Our data structure, called sa-tree (“spatial approximation tree”), is based on approaching the searched objects spatially, that is, getting closer and closer to them, rather than the classic divide-and-conquer approach of other data structures. We analyze our method and show that the number of distance evaluations to search among n objects is sublinear. We show experimentally that the sa-tree is the best existing technique when the metric space is hard to search or the query has low selectivity. These are the most important unsolved cases in real applications. As a practical advantage, our data structure is one of the few that does not need to tune parameters, which makes it appealing for use by non-experts.},
	language = {en},
	number = {1},
	urldate = {2024-04-25},
	journal = {The VLDB Journal},
	author = {Navarro, Gonzalo},
	month = aug,
	year = {2002},
	keywords = {Keywords: Spatial approximation tree – Similarity or proximity search – Spatial and multidimensional search – Multimedia databases},
	pages = {28--46},
	file = {Full Text PDF:/Users/edoriggio/Zotero/storage/J8IY2MI4/Navarro - 2002 - Searching in metric spaces by spatial approximatio.pdf:application/pdf},
}

@article{tellez_singleton_2016,
	title = {Singleton indexes for nearest neighbor search},
	volume = {60},
	issn = {0306-4379},
	url = {https://doi.org/10.1016/j.is.2016.03.003},
	doi = {10.1016/j.is.2016.03.003},
	abstract = {The nearest neighbor search problem is fundamental in computer science, and in spite of the effort of a vast number of research groups, the instances allowing an efficient solution are reduced to databases of objects of small intrinsic dimensions. For intrinsically high-dimensional data, the only possible solution is to compromise and use approximate or probabilistic approaches. For the rest of the instances in the middle, there is an overwhelmingly large number of indexes of claimed good performance. However, the problem of parameter selection makes them unwieldy for use outside of the research community. Even if the indexes can be tuned correctly, either the number of operations for the index construction and tuning is prohibitively large or there are obscure parameters to tune-up. Those restrictions force users from different fields to use brute force to solve the problem in real world instances.In this paper, we present a family of indexing algorithms designed for end users. They require as input, the database, a query sample and the amount of space available. Our building blocks are standard discarding rules, and the indexes will add routing objects such as pivots, hyperplane references or cluster centroids. Those indexes are built incrementally and will self-tune by greedily searching for a global optimum in performance.We experimentally show that using this oblivious strategy our indexes are able to outperform state of the art, manually fine-tuned indexes. For example, our indexes are twice as fast than the fastest alternative (LC, EPT or VPT) for most of our datasets. In the case of LC, the faster alternative for high dimensional datasets, the difference is smaller than 5\%. In the same case, our indexes are at least one order of magnitude faster to build. This superior performance is maintained for large, high dimensional datasets (100 million 12-dimensional objects). In this benchmark, our best index is two times faster than the closest alternative (VPT), six times faster than the majority of indexes, and more than sixty times faster than the sequential scan. HighlightsA description and analysis of the new metric indexing methodology ANNI.A series of indexes based on this methodology.An extensive experimental comparison with the current state of the art.An opensource implementation of all the presented algorithms.},
	number = {C},
	urldate = {2024-04-25},
	journal = {Information Systems},
	author = {Tellez, E.S. and Ruiz, G. and Chavez, E.},
	month = aug,
	year = {2016},
	keywords = {Auto-tuning indexes, Metric indexes, Nearest neighbor search, Pivot selection},
	pages = {50--68},
}

@article{houle_rank-based_2015,
	title = {Rank-{Based} {Similarity} {Search}: {Reducing} the {Dimensional} {Dependence}},
	volume = {37},
	issn = {1939-3539},
	shorttitle = {Rank-{Based} {Similarity} {Search}},
	url = {https://ieeexplore.ieee.org/abstract/document/6866199},
	doi = {10.1109/TPAMI.2014.2343223},
	abstract = {This paper introduces a data structure for k-NN search, the Rank Cover Tree (RCT), whose pruning tests rely solely on the comparison of similarity values; other properties of the underlying space, such as the triangle inequality, are not employed. Objects are selected according to their ranks with respect to the query object, allowing much tighter control on the overall execution costs. A formal theoretical analysis shows that with very high probability, the RCT returns a correct query result in time that depends very competitively on a measure of the intrinsic dimensionality of the data set. The experimental results for the RCT show that non-metric pruning strategies for similarity search can be practical even when the representational dimension of the data is extremely high. They also show that the RCT is capable of meeting or exceeding the level of performance of state-of-the-art methods that make use of metric pruning or other selection tests involving numerical constraints on distance values.},
	number = {1},
	urldate = {2024-04-25},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Houle, Michael E. and Nett, Michael},
	month = jan,
	year = {2015},
	keywords = {Complexity theory, Search problems, Nearest neighbor search, Approximation methods, Data mining, Indexes, intrinsic dimensionality, Measurement, Navigation, rank-based search},
	pages = {136--150},
	file = {IEEE Xplore Abstract Record:/Users/edoriggio/Zotero/storage/S6YBDLYH/6866199.html:text/html;IEEE Xplore Full Text PDF:/Users/edoriggio/Zotero/storage/G4NFXR8Q/Houle and Nett - 2015 - Rank-Based Similarity Search Reducing the Dimensi.pdf:application/pdf},
}

@article{muja_scalable_2014,
	title = {Scalable {Nearest} {Neighbor} {Algorithms} for {High} {Dimensional} {Data}},
	volume = {36},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/abstract/document/6809191},
	doi = {10.1109/TPAMI.2014.2321376},
	abstract = {For many computer vision and machine learning problems, large training sets are key for good performance. However, the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data. We propose new algorithms for approximate nearest neighbor matching and evaluate and compare them with previous algorithms. For matching high dimensional features, we find two algorithms to be the most efficient: the randomized k-d forest and a new algorithm proposed in this paper, the priority search k-means tree. We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature. We show that the optimal nearest neighbor algorithm and its parameters depend on the data set characteristics and describe an automated configuration procedure for finding the best algorithm to search a particular data set. In order to scale to very large data sets that would otherwise not fit in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper. All this research has been released as an open source library called fast library for approximate nearest neighbors (FLANN), which has been incorporated into OpenCV and is now one of the most popular libraries for nearest neighbor matching.},
	number = {11},
	urldate = {2024-04-25},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Muja, Marius and Lowe, David G.},
	month = nov,
	year = {2014},
	keywords = {approximate search, Approximation algorithms, big data, Nearest neighbor search, Approximation methods, algorithm configuration, Clustering algorithms, Computer vision, Machine learning algorithms, Partitioning algorithms, Vegetation},
	pages = {2227--2240},
	file = {IEEE Xplore Abstract Record:/Users/edoriggio/Zotero/storage/VBWRJ2XG/6809191.html:text/html;IEEE Xplore Full Text PDF:/Users/edoriggio/Zotero/storage/J9PYJVMJ/Muja and Lowe - 2014 - Scalable Nearest Neighbor Algorithms for High Dime.pdf:application/pdf},
}

@phdthesis{bogner_evolvability_2020,
	title = {On the evolvability assurance of microservices: metrics, scenarios, and patterns},
	shorttitle = {On the evolvability assurance of microservices},
	abstract = {Context: Fast moving markets and the age of digitization require that software can be quickly changed or extended with new features. The associated quality attribute is referred to as evolvability: the degree of effectiveness and efficiency with which a system can be adapted or extended. Evolvability is especially important for software with frequently changing requirements, e.g. internet-based systems. Several evolvability-related benefits were arguably gained with the rise of service-oriented computing (SOC) that established itself as one of the most important paradigms for distributed systems over the last decade. The implementation of enterprise-wide software landscapes in the style of service-oriented architecture (SOA) prioritizes loose coupling, encapsulation, interoperability, composition, and reuse. In recent years, microservices quickly gained in popularity as an agile, DevOps-focused, and decentralized service-oriented variant with fine-grained services. A key idea here is that small and loosely coupled services that are independently deployable should be easy to change and to replace. Moreover, one of the postulated microservices characteristics is evolutionary design. Problem Statement: While these properties provide a favorable theoretical basis for evolvable systems, they offer no concrete and universally applicable solutions. As with each architectural style, the implementation of a concrete microservice-based system can be of arbitrary quality. Several studies also report that software professionals trust in the foundational maintainability of service orientation and microservices in particular. A blind belief in these qualities without appropriate evolvability assurance can lead to violations of important principles and therefore negatively impact software evolution. In addition to this, very little scientific research has covered the areas of maintenance, evolution, or technical debt of microservices. Objectives: To address this, the aim of this research is to support developers of microservices with appropriate methods, techniques, and tools to evaluate or improve evolvability and to facilitate sustainable long-term development. In particular, we want to provide recommendations and tool support for metric-based as well as scenario-based evaluation. In the context of service-based evolvability, we furthermore want to analyze the effectiveness of patterns and collect relevant antipatterns. Methods: Using empirical methods, we analyzed the industry state of the practice and the academic state of the art, which helped us to identify existing techniques, challenges, and research gaps. Based on these findings, we then designed new evolvability assurance techniques and used additional empirical studies to demonstrate and evaluate their effectiveness. Applied empirical methods were for example surveys, interviews, (systematic) literature studies, or controlled experiments. Contributions: In addition to our analyses of industry practice and scientific literature, we provide contributions in three different areas. With respect to metric-based evolvability evaluation, we identified a set of structural metrics specifically designed for service orientation and analyzed their value for microservices. Subsequently, we designed tool-supported approaches to automatically gather a subset of these metrics from machine-readable RESTful API descriptions and via a distributed tracing mechanism at runtime. In the area of scenario-based evaluation, we developed a tool-supported lightweight method to analyze the evolvability of a service-based system based on hypothetical evolution scenarios. We evaluated the method with a survey (N=40) as well as hands-on interviews (N=7) and improved it further based on the findings. Lastly with respect to patterns and antipatterns, we collected a large set of service-based patterns and analyzed their applicability for microservices. From this initial catalogue, we synthesized a set of candidate evolvability patterns via the proxy of architectural modifiability tactics. The impact of four of these patterns on evolvability was then empirically tested in a controlled experiment (N=69) and with a metric-based analysis. The results suggest that the additional structural complexity introduced by the patterns as well as developers' pattern knowledge have an influence on their effectiveness. As a last contribution, we created a holistic collection of service-based antipatterns for both SOA and microservices and published it in a collaborative repository. Conclusion: Our contributions provide first foundations for a holistic view on the evolvability assurance of microservices and address several perspectives. Metric- and scenario-based evaluation as well as service-based antipatterns can be used to identify "hot spots" while service-based patterns can remediate them and provide means for systematic evolvability construction. All in all, researchers and practitioners in the field of microservices can use our artifacts to analyze and improve the evolvability of their systems as well as to gain a conceptual understanding of service-based evolvability assurance.},
	author = {Bogner, Justus},
	month = jan,
	year = {2020},
	doi = {10.18419/opus-10950},
	file = {Full Text PDF:/Users/edoriggio/Zotero/storage/PJX4HCIB/Bogner - 2020 - On the evolvability assurance of microservices me.pdf:application/pdf},
}

@article{bragilovski_how_2024,
	title = {How do {I} find reusable models?},
	volume = {23},
	issn = {1619-1374},
	url = {https://doi.org/10.1007/s10270-023-01103-7},
	doi = {10.1007/s10270-023-01103-7},
	abstract = {Models play a major role in model-based development and serve as the main artifacts that stakeholders aim to achieve. As it is difficult to develop good-quality models, repositories of models start emerging for reuse purposes. Yet, these repositories face several challenges, such as model representation, scalability, heterogeneity, and how to search for models. In this paper, we aim to address the challenge of querying model repositories by proposing a generic search framework that looks for models that match the intention of the user. The framework is based on a greedy search approach using a similarity function that considers type similarity, structure similarity, and label similarity. We evaluate the framework’s efficiency on different model types: UML class diagrams, Human Know-How, and ME maps. We further compare it with existing alternatives. The evaluation indicates that the framework achieved high performance within a bounded time, and the framework can be adapted to different modeling languages for searching related, reusable models.},
	number = {1},
	journal = {Software and Systems Modeling},
	author = {Bragilovski, Maxim and Stern, Roni and Sturm, Arnon},
	month = feb,
	year = {2024},
	pages = {85--102},
}

@article{canamares_offline_2020,
	title = {Offline evaluation options for recommender systems},
	volume = {23},
	issn = {1573-7659},
	url = {https://doi.org/10.1007/s10791-020-09371-3},
	doi = {10.1007/s10791-020-09371-3},
	abstract = {We undertake a detailed examination of the steps that make up offline experiments for recommender system evaluation, including the manner in which the available ratings are filtered and split into training and test; the selection of a subset of the available users for the evaluation; the choice of strategy to handle the background effects that arise when the system is unable to provide scores for some items or users; the use of either full or condensed output lists for the purposes of scoring; scoring methods themselves, including alternative top-weighted mechanisms for condensed rankings; and the application of statistical testing on a weighted-by-user or weighted-by-volume basis as a mechanism for providing confidence in measured outcomes. We carry out experiments that illustrate the impact that each of these choice points can have on the usefulness of an end-to-end system evaluation, and provide examples of possible pitfalls. In particular, we show that varying the split between training and test data, or changing the evaluation metric, or how target items are selected, or how empty recommendations are dealt with, can give rise to comparisons that are vulnerable to misinterpretation, and may lead to different or even opposite outcomes, depending on the exact combination of settings used.},
	language = {en},
	number = {4},
	urldate = {2024-05-14},
	journal = {Information Retrieval Journal},
	author = {Cañamares, Rocío and Castells, Pablo and Moffat, Alistair},
	month = aug,
	year = {2020},
	keywords = {Effectiveness metric, Evaluation, Experimental design, Recommender systems},
	pages = {387--410},
	file = {Full Text PDF:/Users/edoriggio/Zotero/storage/S7JFADXE/Cañamares et al. - 2020 - Offline evaluation options for recommender systems.pdf:application/pdf},
}

@article{hofmann_online_2016,
	title = {Online {Evaluation} for {Information} {Retrieval}},
	volume = {10},
	issn = {1554-0669},
	url = {https://doi.org/10.1561/1500000051},
	doi = {10.1561/1500000051},
	abstract = {Online evaluation is one of the most common approaches to measure the effectiveness of an information retrieval system. It involves fielding the information retrieval system to real users, and observing these users' interactions in-situ while they engage with the system. This allows actual users with real world information needs to play an important part in assessing retrieval quality. As such, online evaluation complements the common alternative offline evaluation approaches which may provide more easily interpretable outcomes, yet are often less realistic when measuring of quality and actual user experience.In this survey, we provide an overview of online evaluation techniques for information retrieval. We show how online evaluation is used for controlled experiments, segmenting them into experiment designs that allow absolute or relative quality assessments. Our presentation of different metrics further partitions online evaluation based on different sized experimental units commonly of interest: documents, lists and sessions. Additionally, we include an extensive discussion of recent work on data re-use, and experiment estimation based on historical data.A substantial part of this work focuses on practical issues: How to run evaluations in practice, how to select experimental parameters, how to take into account ethical considerations inherent in online evaluations, and limitations that experimenters should be aware of. While most published work on online experimentation today is at large scale in systems with millions of users, we also emphasize that the same techniques can be applied at small scale. To this end, we emphasize recent work that makes it easier to use at smaller scales and encourage studying real-world information seeking in a wide range of scenarios. Finally, we present a summary of the most recent work in the area, and describe open problems, as well as postulating future directions.},
	number = {1},
	urldate = {2024-05-14},
	journal = {Foundations and Trends in Information Retrieval},
	author = {Hofmann, Katja and Li, Lihong and Radlinski, Filip},
	month = jun,
	year = {2016},
	pages = {1--117},
}

@inproceedings{yilmaz_new_2008,
	address = {New York, NY, USA},
	series = {{SIGIR} '08},
	title = {A new rank correlation coefficient for information retrieval},
	isbn = {978-1-60558-164-4},
	url = {https://dl.acm.org/doi/10.1145/1390334.1390435},
	doi = {10.1145/1390334.1390435},
	abstract = {In the field of information retrieval, one is often faced with the problem of computing the correlation between two ranked lists. The most commonly used statistic that quantifies this correlation is Kendall's Τ. Often times, in the information retrieval community, discrepancies among those items having high rankings are more important than those among items having low rankings. The Kendall's Τ statistic, however, does not make such distinctions and equally penalizes errors both at high and low rankings. In this paper, we propose a new rank correlation coefficient, AP correlation (Τap), that is based on average precision and has a probabilistic interpretation. We show that the proposed statistic gives more weight to the errors at high rankings and has nice mathematical properties which make it easy to interpret. We further validate the applicability of the statistic using experimental data.},
	urldate = {2024-05-14},
	booktitle = {Proceedings of the 31st annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
	publisher = {Association for Computing Machinery},
	author = {Yilmaz, Emine and Aslam, Javed A. and Robertson, Stephen},
	month = jul,
	year = {2008},
	keywords = {average precision, evaluation, Kendall's tau, rank correlation},
	pages = {587--594},
	file = {Full Text PDF:/Users/edoriggio/Zotero/storage/W7PG7CCN/Yilmaz et al. - 2008 - A new rank correlation coefficient for information.pdf:application/pdf},
}
